{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Saving-and-restoring-data-with-Python\" data-toc-modified-id=\"Saving-and-restoring-data-with-Python-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Saving and restoring data with Python</a></span><ul class=\"toc-item\"><li><span><a href=\"#np.save\" data-toc-modified-id=\"np.save-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><code>np.save</code></a></span></li><li><span><a href=\"#MacOSFile\" data-toc-modified-id=\"MacOSFile-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span><code>MacOSFile</code></a></span></li><li><span><a href=\"#np.savez\" data-toc-modified-id=\"np.savez-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span><code>np.savez</code></a></span></li><li><span><a href=\"#h5py\" data-toc-modified-id=\"h5py-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>h5py</a></span><ul class=\"toc-item\"><li><span><a href=\"#h5py\" data-toc-modified-id=\"h5py-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span><code>h5py</code></a></span></li><li><span><a href=\"#DeepDish\" data-toc-modified-id=\"DeepDish-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>DeepDish</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dealing-with-large-datasets:\" data-toc-modified-id=\"Dealing-with-large-datasets:-1.4.2.1\"><span class=\"toc-item-num\">1.4.2.1&nbsp;&nbsp;</span>Dealing with large datasets:</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**@juliaroquette: In this notebook I present some ways of saving data in Python formating. Here, by Python format I mean saving arrays, dictionaries, lists, etc inside a single file**\n",
    "\n",
    "**- last modified: 17May19**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and restoring data with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supose we have a dictionary of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['d1', 'd2', 'd3'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1={\"a\":np.random.random(size=100),\"b\":np.random.random(size=100),\"c\":np.random.random(size=100)}\n",
    "d2={\"a\":np.random.random(size=100),\"b\":np.random.random(size=100),\"c\":np.random.random(size=100)}\n",
    "d3={\"a\":np.random.random(size=100),\"b\":np.random.random(size=100),\"c\":np.random.random(size=100)}\n",
    "D={\"d1\":d1,\"d2\":d2,\"d3\":d3}\n",
    "D.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a second version of it which includes some very large array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['d1', 'd2', 'd3', 'd4'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L={\"d1\":d1,\"d2\":d2,\"d3\":d3,\"d4\":bytearray(8 * 1000 * 1000 * 1000)}\n",
    "L.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `np.save`\n",
    "Documentation: https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.save.html#numpy.save\n",
    "\n",
    "To save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('save',D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466 µs ± 25.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.save('save',D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To restore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1=np.load('save.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317 µs ± 10.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit D1=np.load('save.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems: It doesn't re-store the Dictionary with the original structure. For example, one can not use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-dae96eb2261b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mD1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "D1.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['d1', 'd2', 'd3'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D1.item().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Major issue: In Python 3, `np.save` crashes for large tables. For example, if we try to save the large version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "cannot serialize a string larger than 4GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fbf163b453c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saveL'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[0;32m--> 521\u001b[0;31m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: cannot serialize a string larger than 4GiB"
     ]
    }
   ],
   "source": [
    "np.save('saveL',L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MacOSFile`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StackOverflow presents a workaround this problem using `pickle` directly: https://stackoverflow.com/questions/31468117/python-3-can-pickle-handle-byte-objects-larger-than-4gb\n",
    "Which was implemented criating a class, such as bellow, saved in `.../work/prog/MacOSFile`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class MacOSFile(object):\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return getattr(self.f, item)\n",
    "\n",
    "    def read(self, n):\n",
    "        # print(\"reading total_bytes=%s\" % n, flush=True)\n",
    "        if n >= (1 << 31):\n",
    "            buffer = bytearray(n)\n",
    "            idx = 0\n",
    "            while idx < n:\n",
    "                batch_size = min(n - idx, 1 << 31 - 1)\n",
    "                # print(\"reading bytes [%s,%s)...\" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "                buffer[idx:idx + batch_size] = self.f.read(batch_size)\n",
    "                # print(\"done.\", flush=True)\n",
    "                idx += batch_size\n",
    "            return buffer\n",
    "        return self.f.read(n)\n",
    "\n",
    "    def write(self, buffer):\n",
    "        n = len(buffer)\n",
    "        print(\"writing total_bytes=%s...\" % n, flush=True)\n",
    "        idx = 0\n",
    "        while idx < n:\n",
    "            batch_size = min(n - idx, 1 << 31 - 1)\n",
    "            print(\"writing bytes [%s, %s)... \" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "            self.f.write(buffer[idx:idx + batch_size])\n",
    "            print(\"done.\", flush=True)\n",
    "            idx += batch_size\n",
    "\n",
    "\n",
    "def pickle_dump(obj, file_path):\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        return pickle.dump(obj, MacOSFile(f), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def pickle_load(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(MacOSFile(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that can be used by importing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing total_bytes=7708...\n",
      "writing bytes [0, 7708)... done.\n",
      "writing total_bytes=8000000000...\n",
      "writing bytes [0, 1073741824)... done.\n",
      "writing bytes [1073741824, 2147483648)... done.\n",
      "writing bytes [2147483648, 3221225472)... done.\n",
      "writing bytes [3221225472, 4294967296)... done.\n",
      "writing bytes [4294967296, 5368709120)... done.\n",
      "writing bytes [5368709120, 6442450944)... done.\n",
      "writing bytes [6442450944, 7516192768)... done.\n",
      "writing bytes [7516192768, 8000000000)... done.\n",
      "writing total_bytes=16...\n",
      "writing bytes [0, 16)... done.\n"
     ]
    }
   ],
   "source": [
    "import MacOSFile\n",
    "MacOSFile.pickle_dump(L,'D1save.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing total_bytes=7708...\n",
      "writing bytes [0, 7708)... done.\n",
      "writing total_bytes=8000000000...\n",
      "writing bytes [0, 1073741824)... done.\n",
      "writing bytes [1073741824, 2147483648)... done.\n",
      "writing bytes [2147483648, 3221225472)... done.\n",
      "writing bytes [3221225472, 4294967296)... done.\n",
      "writing bytes [4294967296, 5368709120)... done.\n",
      "writing bytes [5368709120, 6442450944)... done.\n",
      "writing bytes [6442450944, 7516192768)... done.\n",
      "writing bytes [7516192768, 8000000000)... done.\n",
      "writing total_bytes=16...\n",
      "writing bytes [0, 16)... done.\n",
      "writing total_bytes=7708...\n",
      "writing bytes [0, 7708)... done.\n",
      "writing total_bytes=8000000000...\n",
      "writing bytes [0, 1073741824)... done.\n",
      "writing bytes [1073741824, 2147483648)... done.\n",
      "writing bytes [2147483648, 3221225472)... done.\n",
      "writing bytes [3221225472, 4294967296)... done.\n",
      "writing bytes [4294967296, 5368709120)... done.\n",
      "writing bytes [5368709120, 6442450944)... done.\n",
      "writing bytes [6442450944, 7516192768)... done.\n",
      "writing bytes [7516192768, 8000000000)... done.\n",
      "writing total_bytes=16...\n",
      "writing bytes [0, 16)... done.\n",
      "writing total_bytes=7708...\n",
      "writing bytes [0, 7708)... done.\n",
      "writing total_bytes=8000000000...\n",
      "writing bytes [0, 1073741824)... done.\n",
      "writing bytes [1073741824, 2147483648)... done.\n",
      "writing bytes [2147483648, 3221225472)... done.\n",
      "writing bytes [3221225472, 4294967296)... done.\n",
      "writing bytes [4294967296, 5368709120)... done.\n",
      "writing bytes [5368709120, 6442450944)... done.\n",
      "writing bytes [6442450944, 7516192768)... done.\n",
      "writing bytes [7516192768, 8000000000)... done.\n",
      "writing total_bytes=16...\n",
      "writing bytes [0, 16)... done.\n",
      "writing total_bytes=7708...\n",
      "writing bytes [0, 7708)... done.\n",
      "writing total_bytes=8000000000...\n",
      "writing bytes [0, 1073741824)... done.\n",
      "writing bytes [1073741824, 2147483648)... done.\n",
      "writing bytes [2147483648, 3221225472)... done.\n",
      "writing bytes [3221225472, 4294967296)... done.\n",
      "writing bytes [4294967296, 5368709120)... done.\n",
      "writing bytes [5368709120, 6442450944)... done.\n",
      "writing bytes [6442450944, 7516192768)... done.\n",
      "writing bytes [7516192768, 8000000000)... done.\n",
      "writing total_bytes=16...\n",
      "writing bytes [0, 16)... done.\n",
      "writing total_bytes=7708...\n",
      "writing bytes [0, 7708)... done.\n",
      "writing total_bytes=8000000000...\n",
      "writing bytes [0, 1073741824)... done.\n",
      "writing bytes [1073741824, 2147483648)... done.\n",
      "writing bytes [2147483648, 3221225472)... done.\n",
      "writing bytes [3221225472, 4294967296)... done.\n",
      "writing bytes [4294967296, 5368709120)... done.\n",
      "writing bytes [5368709120, 6442450944)... done.\n",
      "writing bytes [6442450944, 7516192768)... done.\n",
      "writing bytes [7516192768, 8000000000)... done.\n",
      "writing total_bytes=16...\n",
      "writing bytes [0, 16)... done.\n",
      "writing total_bytes=7708...\n",
      "writing bytes [0, 7708)... done.\n",
      "writing total_bytes=8000000000...\n",
      "writing bytes [0, 1073741824)... done.\n",
      "writing bytes [1073741824, 2147483648)... done.\n",
      "writing bytes [2147483648, 3221225472)... done.\n",
      "writing bytes [3221225472, 4294967296)... done.\n",
      "writing bytes [4294967296, 5368709120)... done.\n",
      "writing bytes [5368709120, 6442450944)... done.\n",
      "writing bytes [6442450944, 7516192768)... done.\n",
      "writing bytes [7516192768, 8000000000)... done.\n",
      "writing total_bytes=16...\n",
      "writing bytes [0, 16)... done.\n",
      "writing total_bytes=7708...\n",
      "writing bytes [0, 7708)... done.\n",
      "writing total_bytes=8000000000...\n",
      "writing bytes [0, 1073741824)... done.\n",
      "writing bytes [1073741824, 2147483648)... done.\n",
      "writing bytes [2147483648, 3221225472)... done.\n",
      "writing bytes [3221225472, 4294967296)... done.\n",
      "writing bytes [4294967296, 5368709120)... done.\n",
      "writing bytes [5368709120, 6442450944)... done.\n",
      "writing bytes [6442450944, 7516192768)... done.\n",
      "writing bytes [7516192768, 8000000000)... done.\n",
      "writing total_bytes=16...\n",
      "writing bytes [0, 16)... done.\n",
      "writing total_bytes=7708...\n",
      "writing bytes [0, 7708)... done.\n",
      "writing total_bytes=8000000000...\n",
      "writing bytes [0, 1073741824)... done.\n",
      "writing bytes [1073741824, 2147483648)... done.\n",
      "writing bytes [2147483648, 3221225472)... done.\n",
      "writing bytes [3221225472, 4294967296)... done.\n",
      "writing bytes [4294967296, 5368709120)... done.\n",
      "writing bytes [5368709120, 6442450944)... done.\n",
      "writing bytes [6442450944, 7516192768)... done.\n",
      "writing bytes [7516192768, 8000000000)... done.\n",
      "writing total_bytes=16...\n",
      "writing bytes [0, 16)... done.\n",
      "35.7 s ± 1.86 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit MacOSFile.pickle_dump(L,'D1save.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1=MacOSFile.pickle_load('D1save.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.2 s ± 1.96 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit MacOSFile.pickle_load('D1save.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it does solve the problem of restoring the dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['d1', 'd2', 'd3', 'd4'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `np.savez`\n",
    "Documentation: https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.savez.html\n",
    "\n",
    "To open:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('savez',D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34 ms ± 671 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.savez('savez',D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2=np.load('savez.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.7 µs ± 872 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.load('savez.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arr_0']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D2.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2d=D2['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-818dccb9fdd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mD2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "D2d.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also doesn't work for large datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "cannot serialize a string larger than 4GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f4bc1851dfa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'savez'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavez\u001b[0;34m(file, *args, **kwds)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m     \u001b[0m_savez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m_savez\u001b[0;34m(file, args, kwds, compress, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    704\u001b[0m                 format.write_array(fid, val,\n\u001b[1;32m    705\u001b[0m                                    \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                                    pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    707\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;31m# Stage arrays in a temporary file on disk, before writing to zip.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: cannot serialize a string larger than 4GiB"
     ]
    }
   ],
   "source": [
    "np.savez('savez',L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h5py\n",
    "\n",
    "Other workaround found in StackOverflow (https://stackoverflow.com/questions/30811918/saving-dictionary-of-numpy-arrays/30812215) is to use the `HDF5` format (https://en.wikipedia.org/wiki/Hierarchical_Data_Format) \n",
    "\n",
    "\n",
    "I found two packages in `Python` for that and will compare them.\n",
    "\n",
    "Some discussion about the two formats: http://tdeboissiere.github.io/h5py-vs-npz.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `h5py`\n",
    "(Documentation http://docs.h5py.org/en/latest/index.html) It looks to me as a primitive package to work with HDF5 - a bit like using only built-in functions of python to write an ascii file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepDish\n",
    "\n",
    "(http://deepdish.readthedocs.io/en/latest/tutorials.html)\n",
    "(https://github.com/uchicago-cs/deepdish) \n",
    "\n",
    "Which is a \"series of data science tools from the University of Chicago\" including a `save/load` function similar to `np.save` or `pickle` and that deals with the `HDF5` format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepdish\n",
      "  Using cached https://files.pythonhosted.org/packages/6e/39/2a47c852651982bc5eb39212ac110284dd20126bdc7b49bde401a0139f5d/deepdish-0.3.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tables in /anaconda3/lib/python3.7/site-packages (from deepdish) (3.4.4)\n",
      "Requirement already satisfied: scipy in /anaconda3/lib/python3.7/site-packages (from deepdish) (1.1.0)\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (from deepdish) (1.15.4)\n",
      "Requirement already satisfied: numexpr>=2.5.2 in /anaconda3/lib/python3.7/site-packages (from tables->deepdish) (2.6.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /anaconda3/lib/python3.7/site-packages (from tables->deepdish) (1.12.0)\n",
      "Installing collected packages: deepdish\n",
      "Successfully installed deepdish-0.3.6\n"
     ]
    }
   ],
   "source": [
    "!pip install deepdish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepdish as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.io.save('saveDdd',D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3 ms ± 466 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dd.io.save('saveDdd',D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "``saveDdd.h5`` does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-383e7840449c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saveDdd.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/deepdish/io/hdf5io.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, group, sel, unpack)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \"\"\"\n\u001b[0;32m--> 635\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh5file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0mpathtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# dict to keep track of objects already loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tables/file.py\u001b[0m in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m# Finally, create the File instance, and return it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_uep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tables/file.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0;31m# Now, it is time to initialize the File extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;31m# Check filters and set PyTables format version for new files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtables/hdf5extension.pyx\u001b[0m in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tables/utils.py\u001b[0m in \u001b[0;36mcheck_file_access\u001b[0;34m(filename, mode)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# The file should be readable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF_OK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"``%s`` does not exist\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"``%s`` is not a regular file\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ``saveDdd.h5`` does not exist"
     ]
    }
   ],
   "source": [
    "Ddd=dd.io.load('saveDdd.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit dd.io.load('saveDdd.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ddd.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `deepdish` package deals well with the dictionary structure, but it crashes for datasets larger than 4GB.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Larger dataset version of L1 saved with `MacOSFile` has 7.5G\n",
    "\n",
    "Smaller dataset version saved with `np.save` has 12K\n",
    "\n",
    "Smaller dataset version saved wit `np.savez` has 12K\n",
    "\n",
    "\n",
    "Medium dataset version save with `MacOSFile` has 1.6G, took 19.134106159210205s to be saved and 9.956901788711548s to be re-opened.\n",
    "Medium dataset version save with `MacOSFile` has 1.6G, took 83.79772400856018s to be saved and 0.1864032745361328s to be re-opened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with large datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Deepdish` seems to be the fastest option to reload datasets, but the conclusion here is that as long as `DeepDish` is unable of dealing with files larger than 2GB, the best alternative is to keep using the `MacOSFile` package"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
